{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file collates the project into a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wfdb\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "from scipy.io import loadmat\n",
    "import scipy.signal as signal\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pywt\n",
    "import random\n",
    "import sklearn.model_selection\n",
    "import sklearn.ensemble\n",
    "import scipy.stats\n",
    "import tsfresh\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below defines the functions used for preprocessing the signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_baseline_wander(ecg_signal, fs, cutoff=0.5):\n",
    "    b, a = signal.butter(1, cutoff / (0.5 * fs), btype='highpass')\n",
    "    return signal.filtfilt(b, a, ecg_signal)\n",
    "\n",
    "def low_pass_filter(ecg_signal, fs, cutoff=40):\n",
    "    b, a = signal.butter(4, cutoff / (0.5 * fs), btype='low')\n",
    "    return signal.filtfilt(b, a, ecg_signal)\n",
    "\n",
    "def notch_filter(ecg_signal, fs, notch_freq=50, quality_factor=30):\n",
    "    b, a = signal.iirnotch(notch_freq / (0.5 * fs), quality_factor)\n",
    "    return signal.filtfilt(b, a, ecg_signal)\n",
    "\n",
    "def wavelet_denoising(ecg_signal, wavelet='db6', level=3):\n",
    "    coeffs = pywt.wavedec(ecg_signal, wavelet, level=level)\n",
    "    sigma = np.median(np.abs(coeffs[-1])) / 0.6745  # Estimating noise level\n",
    "    uthresh = sigma * np.sqrt(2 * np.log(len(ecg_signal)))\n",
    "    denoised_coeffs = list(map(lambda x: pywt.threshold(x, uthresh, mode='soft'), coeffs))\n",
    "    return pywt.waverec(denoised_coeffs, wavelet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below gets the file path to all the files for processing, adding it to a list called mat files. It then loops through the paths in this list, reading it in, and then altering the files in-place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ecg_data = os.path.join(os.getcwd(), 'processed_ecg_signals_2/WFDBRecords')\n",
    "\n",
    "mat_files = []\n",
    "hea_files = []\n",
    "\n",
    "for root, dirs, files in os.walk(ecg_data):\n",
    "    for file in files:\n",
    "        if file.endswith('.hea'):\n",
    "            hea_files.append(os.path.join(root, file))\n",
    "        elif file.endswith('.mat'):\n",
    "            mat_files.append(os.path.join(root, file))\n",
    "\n",
    "for file in mat_files:\n",
    "    try: \n",
    "        base_name = os.path.splitext(os.path.basename(file))[0]\n",
    "\n",
    "        \n",
    "        mat_path = os.path.join(os.path.dirname(file), base_name)\n",
    "\n",
    "            \n",
    "        x = wfdb.rdrecord(mat_path)\n",
    "        fs = x.fs\n",
    "\n",
    "        for i in range(0,12):\n",
    "            y = x.p_signal[:,i]\n",
    "            z = remove_baseline_wander(y, fs)\n",
    "            a = low_pass_filter(z,fs)\n",
    "            b = notch_filter(a,fs)\n",
    "            c = wavelet_denoising(b)\n",
    "            x.p_signal[:,i] = c\n",
    "\n",
    "        var = x.p_signal.T #transpose\n",
    "        output_path = os.path.join(os.path.dirname(file), base_name + '.mat')  # Save in original directory\n",
    "        sio.savemat(output_path, {'my_matrix': var})  # Save the modified file\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file}\")\n",
    "    except ValueError:\n",
    "        print(\"value error\")\n",
    "    except IndexError:\n",
    "        print(f\"Index error processing file {file}. Check the signal dimensions.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below collates each record in a dataframe, with its corresponding conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records_list = []\n",
    "conditions_list = []\n",
    "\n",
    "mapping_file = os.path.join(os.getcwd(), 'processed_ecg_signals_2/ConditionNames_SNOMED-CT.csv')\n",
    "mapping_df = pd.read_csv(mapping_file)\n",
    "mapping_dict = dict(zip(mapping_df['Snomed_CT'], mapping_df['Full Name']))\n",
    "\n",
    "\n",
    "\n",
    "for hea_file, mat_file in zip(hea_files, mat_files):\n",
    "    base_name_hea = os.path.splitext(os.path.basename(hea_file))[0]\n",
    "    \n",
    "    header_path = os.path.join(os.path.dirname(hea_file), base_name_hea)\n",
    "\n",
    "    base_name_mat = os.path.splitext(os.path.basename(mat_file))[0]\n",
    "    \n",
    "    mat_path = os.path.join(os.path.dirname(mat_file), base_name_mat)\n",
    "    \n",
    "    try:\n",
    "        record = wfdb.rdheader(header_path)\n",
    "\n",
    "        mat_data = loadmat(mat_path)\n",
    "\n",
    "        data_matrix = mat_data['my_matrix']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #extracting comments\n",
    "        comments = record.comments[2].split(':')\n",
    "        b = comments[1]\n",
    "        b = b.split(\",\")\n",
    "        diagnosis_code = []\n",
    "        for x in b:\n",
    "            diagnosis_code.append(int(x))\n",
    "\n",
    "        diagnosis_descriptions = [mapping_dict.get(x, 'Normal') for x in diagnosis_code]\n",
    "\n",
    "        \n",
    "        records_list.append((record, data_matrix))\n",
    "        conditions_list.append(diagnosis_descriptions)\n",
    "       \n",
    "    except FileNotFoundError:\n",
    "        print(f'File not found: {hea_file}')\n",
    "    except ValueError as e:\n",
    "        print(f\"Error downloading {record}: {e}\")\n",
    "    except KeyError as k:\n",
    "        print(f\"Key error: {record}: {k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code filters the conditions for the stratification and then stratifies them into the training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_conditions = []\n",
    "\n",
    "for conditions in conditions_list:\n",
    "    if len(conditions) > 1:\n",
    "        selected_condition = random.choice(conditions) # this makes a random choice out of the conditions as to the one to use\n",
    "    else:\n",
    "        selected_condition = conditions[0]\n",
    "    filtered_conditions.append(selected_condition)\n",
    "\n",
    "condition_counts = Counter(filtered_conditions)\n",
    "print(condition_counts)\n",
    "\n",
    "conditions_to_remove = {condition for condition, count in condition_counts.items() if count == 1}\n",
    "\n",
    "filtered_conditions_final = []\n",
    "filtered_records = []\n",
    "\n",
    "\n",
    "for record, condition in zip(records_list, filtered_conditions):\n",
    "    if condition not in conditions_to_remove:\n",
    "        filtered_records.append(record)\n",
    "        filtered_conditions_final.append(condition)\n",
    "\n",
    "condition_counts_2 = Counter(filtered_conditions_final)\n",
    "\n",
    "print(condition_counts_2)\n",
    "\n",
    "\n",
    "train_records, test_records, train_conditions, test_conditions = train_test_split(\n",
    "    filtered_records, \n",
    "    filtered_conditions_final, \n",
    "    test_size=0.2, \n",
    "    stratify=filtered_conditions_final, \n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code saves the test and training records in two separate folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hea, mat in test_records:\n",
    "    base_name = hea.record_name\n",
    "    hea_file = os.path.join(base_name + '.hea')\n",
    "    mat_file = os.path.join(base_name + '.mat')\n",
    "\n",
    "    output_path = os.path.join('test_data', base_name + '.mat')  \n",
    "    sio.savemat(output_path, {'my_matrix': mat})  \n",
    "\n",
    "for hea, mat in train_records:\n",
    "    base_name = hea.record_name\n",
    "    hea_file = os.path.join(base_name + '.hea')\n",
    "    mat_file = os.path.join(base_name + '.mat')\n",
    "\n",
    "    output_path = os.path.join('train_data', base_name + '.mat')  \n",
    "    sio.savemat(output_path, {'my_matrix': mat})\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
